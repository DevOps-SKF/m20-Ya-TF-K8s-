terraform plan -out=plan -no-color | Tee-Object plan.log

An execution plan has been generated and is shown below.
Resource actions are indicated with the following symbols:
  + create

Terraform will perform the following actions:

  # local_file.vm-master_ip will be created
  + resource "local_file" "vm-master_ip" {
      + content              = (known after apply)
      + directory_permission = "0777"
      + file_permission      = "0777"
      + filename             = "vm-master_ip.txt"
      + id                   = (known after apply)
    }

  # local_file.vm-worker_ip will be created
  + resource "local_file" "vm-worker_ip" {
      + content              = (known after apply)
      + directory_permission = "0777"
      + file_permission      = "0777"
      + filename             = "vm-worker_ip.txt"
      + id                   = (known after apply)
    }

  # yandex_compute_instance.vmMaster will be created
  + resource "yandex_compute_instance" "vmMaster" {
      + created_at                = (known after apply)
      + folder_id                 = (known after apply)
      + fqdn                      = (known after apply)
      + hostname                  = "vm-master"
      + id                        = (known after apply)
      + labels                    = {
          + "purpose" = "k8s"
        }
      + metadata                  = {
          + "ssh-keys" = "ubuntu:ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQDTVx0/OQ/vLodJbf15sPp5EbqPd2KZlHUrltsRO0qxIJu8KWb5QkrZ+TfPWNV5P8yzbgLgKZewiPZIiGClnkXKBUsffOiFrGDBtXzO5seEjuNYEYB+L4nekjSrvDMQmxZHUeR94y6OVVX+EYiS1/CwUOSdTYUxK7VWRSJM3tVFMPnQwoSsydIKpfgsdvwnZ1Nz3J9u4q1RYzQ1271q8s8M6kRKlt0gbNABIuY7PETEHACk6MrYF4VggSSQkS+g/y6ixU3vwktvphsRKS78j10uu2LY4ut0r9npuCl4wxAINoWRELPh9d9jSp7SLGZXzaFjmACuslTI8oF4PsU10YRen2uTFggUzhA08f8ZrJXk7jA7B2veeEz+XexLlH0TMfywZw9sn5qDq0w//UKDb2hh9O5MQJWHMLSudGDgzUedB2UdeI26BhyYl/GUzE20SzH4wN6kLWyKh3C4PvSpEBt4zSzQZSJqMxrHpWaYmkAX3as4lk6U1EH87WCbxv7BUJc= generated-by-azure"
        }
      + name                      = "vm-master"
      + network_acceleration_type = "standard"
      + platform_id               = "standard-v2"
      + service_account_id        = (known after apply)
      + status                    = (known after apply)
      + zone                      = "ru-central1-a"

      + boot_disk {
          + auto_delete = true
          + device_name = "k8s-master"
          + disk_id     = (known after apply)
          + mode        = (known after apply)

          + initialize_params {
              + description = (known after apply)
              + image_id    = "fd8vmcue7aajpmeo39kk"
              + name        = "k8s-master"
              + size        = 32
              + snapshot_id = (known after apply)
              + type        = "network-hdd"
            }
        }

      + network_interface {
          + index              = (known after apply)
          + ip_address         = "172.19.0.11"
          + ipv4               = true
          + ipv6               = false
          + ipv6_address       = (known after apply)
          + mac_address        = (known after apply)
          + nat                = true
          + nat_ip_address     = (known after apply)
          + nat_ip_version     = (known after apply)
          + security_group_ids = (known after apply)
          + subnet_id          = (known after apply)
        }

      + resources {
          + core_fraction = 5
          + cores         = 2
          + memory        = 2
        }

      + scheduling_policy {
          + preemptible = true
        }
    }

  # yandex_compute_instance.vmWorker will be created
  + resource "yandex_compute_instance" "vmWorker" {
      + created_at                = (known after apply)
      + folder_id                 = (known after apply)
      + fqdn                      = (known after apply)
      + hostname                  = "vm-worker"
      + id                        = (known after apply)
      + labels                    = {
          + "purpose" = "k8s"
        }
      + metadata                  = {
          + "ssh-keys" = "ubuntu:ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQDTVx0/OQ/vLodJbf15sPp5EbqPd2KZlHUrltsRO0qxIJu8KWb5QkrZ+TfPWNV5P8yzbgLgKZewiPZIiGClnkXKBUsffOiFrGDBtXzO5seEjuNYEYB+L4nekjSrvDMQmxZHUeR94y6OVVX+EYiS1/CwUOSdTYUxK7VWRSJM3tVFMPnQwoSsydIKpfgsdvwnZ1Nz3J9u4q1RYzQ1271q8s8M6kRKlt0gbNABIuY7PETEHACk6MrYF4VggSSQkS+g/y6ixU3vwktvphsRKS78j10uu2LY4ut0r9npuCl4wxAINoWRELPh9d9jSp7SLGZXzaFjmACuslTI8oF4PsU10YRen2uTFggUzhA08f8ZrJXk7jA7B2veeEz+XexLlH0TMfywZw9sn5qDq0w//UKDb2hh9O5MQJWHMLSudGDgzUedB2UdeI26BhyYl/GUzE20SzH4wN6kLWyKh3C4PvSpEBt4zSzQZSJqMxrHpWaYmkAX3as4lk6U1EH87WCbxv7BUJc= generated-by-azure"
        }
      + name                      = "vm-worker"
      + network_acceleration_type = "standard"
      + platform_id               = "standard-v2"
      + service_account_id        = (known after apply)
      + status                    = (known after apply)
      + zone                      = "ru-central1-a"

      + boot_disk {
          + auto_delete = true
          + device_name = "k8s-worker"
          + disk_id     = (known after apply)
          + mode        = (known after apply)

          + initialize_params {
              + description = (known after apply)
              + image_id    = "fd8vmcue7aajpmeo39kk"
              + name        = "k8s-worker"
              + size        = 32
              + snapshot_id = (known after apply)
              + type        = "network-hdd"
            }
        }

      + network_interface {
          + index              = (known after apply)
          + ip_address         = "172.19.0.12"
          + ipv4               = true
          + ipv6               = false
          + ipv6_address       = (known after apply)
          + mac_address        = (known after apply)
          + nat                = true
          + nat_ip_address     = (known after apply)
          + nat_ip_version     = (known after apply)
          + security_group_ids = (known after apply)
          + subnet_id          = (known after apply)
        }

      + resources {
          + core_fraction = 5
          + cores         = 2
          + memory        = 2
        }

      + scheduling_policy {
          + preemptible = true
        }
    }

  # yandex_vpc_network.k8s will be created
  + resource "yandex_vpc_network" "k8s" {
      + created_at                = (known after apply)
      + default_security_group_id = (known after apply)
      + folder_id                 = (known after apply)
      + id                        = (known after apply)
      + labels                    = {
          + "purpose" = "k8s"
        }
      + name                      = "vpcSKF"
      + subnet_ids                = (known after apply)
    }

  # yandex_vpc_subnet.k8s will be created
  + resource "yandex_vpc_subnet" "k8s" {
      + created_at     = (known after apply)
      + folder_id      = (known after apply)
      + id             = (known after apply)
      + labels         = {
          + "purpose" = "k8s"
        }
      + name           = "k8s-172.19.0"
      + network_id     = (known after apply)
      + v4_cidr_blocks = [
          + "172.19.0.0/24",
        ]
      + v6_cidr_blocks = (known after apply)
      + zone           = "ru-central1-a"
    }

Plan: 6 to add, 0 to change, 0 to destroy.

Changes to Outputs:
  + vm-master_ip = (known after apply)
  + vm-worker_ip = (known after apply)

------------------------------------------------------------------------

This plan was saved to: plan

To perform exactly these actions, run the following command to apply:
    terraform apply "plan"

